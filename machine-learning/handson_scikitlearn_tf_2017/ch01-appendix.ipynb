{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The usage of random.seed()\n",
    "\n",
    "如果使用了 random.seed()，那么每次所产生的随机数将与前面一次的相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88389311  0.19586502  0.35753652 -2.34326191 -1.08483259]\n",
      "**\n",
      "[ 0.55969629  0.93946935 -0.97848104  0.50309684  0.40641447]\n",
      "**\n",
      "[ 0.32346101 -0.49341088 -0.79201679 -0.84236793 -1.27950266]\n",
      "**\n",
      "[ 0.24571517 -0.0441948   1.56763255  1.05110868  0.40636843]\n",
      "**\n",
      "[-0.1686461  -3.18970279  1.12013226  1.33277821 -0.24333877]\n",
      "**\n",
      "[-0.13003071 -0.10901737  1.55618644  0.12877835 -2.06694872]\n",
      "**\n",
      "[-0.88549315 -1.10457948  0.93286635  2.059838   -0.93493796]\n",
      "**\n",
      "[-1.61299022  0.52706972 -1.55110074  0.32961334 -1.13652654]\n",
      "**\n",
      "[-0.3384906   0.32097078 -0.60230802  1.54472836  0.64703408]\n",
      "**\n",
      "[0.59321721 0.4380245  1.35778902 1.20451128 1.35179619]\n",
      "**\n",
      "[ 4.93437236e-01 -2.70436525e+00 -5.55185797e-01  1.50856026e-03\n",
      "  8.57093817e-01]\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(20)\n",
    "\n",
    "runningtimes=0\n",
    "while (runningtimes <=10):\n",
    "    x = np.random.randn(5)\n",
    "    print(x)\n",
    "    print('**')\n",
    "    runningtimes=runningtimes+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n",
      "[ 0.44122749 -0.33087015  2.43077119 -0.25209213  0.10960984]\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "runningtimes=0\n",
    "while (runningtimes <=10):\n",
    "    np.random.seed(5)\n",
    "    x = np.random.randn(5)\n",
    "    print(x)\n",
    "    print('**')\n",
    "    runningtimes=runningtimes+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under stand how path is joined\n",
    "\n",
    "Here, I use a simple example to show how we generate a path in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/001/300.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ROOT_DIR = '.'\n",
    "CHAPTER_ID = '001'\n",
    "fig_id = '300'\n",
    "path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy.c_ function\n",
    "Translates slice objects to *concatenation* along the *second* axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "*******\n",
      "[[1 2 3 4 5 6]]\n",
      "*******\n",
      "[[1 2 3 0 0 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr1 = np.c_[np.array([1,2,3]), np.array([4,5,6])]\n",
    "print(arr1)\n",
    "print('*******')\n",
    "\n",
    "arr2 = np.c_[np.array([[1,2,3]]), np.array([[4,5,6]])]\n",
    "print(arr2)\n",
    "print('*******')\n",
    "\n",
    "arr3 = np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n",
    "print(arr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coef_ and intercept_\n",
    "\n",
    "- coef_: array, shape (n_features, ) or (n_targets, n_features)\n",
    "Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), \n",
    "this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features.\n",
    "\n",
    "- intercept_ : array\n",
    "Independent term in the linear model.截距\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.linear_model.Ridge\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "Linear least squares with l2 regularization.\n",
    "\n",
    "Minimizes the objective function(最小化目标函数，其实就是最小化cost function):\n",
    "\n",
    "||y - Xw||^2_2 + alpha * ||w||^2_2\n",
    "This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. \n",
    "\n",
    "**Note that alpha is important to fine-tune the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization Methods\n",
    "\n",
    "Ref: https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c Original author: \n",
    "\n",
    "In order to create less complex (parsimonious) model when you have a large number of features in your dataset, some of the Regularization techniques used to address over-fitting and feature selection are:\n",
    "\n",
    "- L1 Regularization\n",
    "\n",
    "- L2 Regularization\n",
    "\n",
    "A regression model that uses L1 regularization technique is called Lasso (Least Absolute Shrinkage and Selection Operator) Regression and model which uses L2 is called Ridge Regression. \n",
    "\n",
    "The key difference between these two is the penalty term.\n",
    "\n",
    "Ridge regression adds \"squared magnitude\" of coefficient as penalty term to the loss function. Here the term in red represents L2 regularization element.\n",
    "\n",
    "![Cost function of L2 regularization](ch01/L2-regularization-cost.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: this fuction was rendered in Latex), the source code for latex is):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\sum_{i=1}^n (y_i - \\sum_{j=1}^p x_{ij}\\beta_{j})^2 + {\\color[rgb]{0.986246,0.007121,0.027434}\\lambda\\sum_{j=1}^p\\beta_j^2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, if lambda is zero then you can imagine we get back OLS (Ordinary least squares). However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.\n",
    "\n",
    "**Lasso Regression** adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
    "\n",
    "![Cost function of L1 regularization](ch01/L1-regularization-cost.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: this fuction was rendered in Latex), the source code for latex is):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\sum_{i=1}^n (y_i - \\sum_{j=1}^p x_{ij}\\beta_{j})^2 + {\\color[rgb]{0.986246,0.007121,0.027434}\\lambda\\sum_{j=1}^p|\\beta_j|}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The key difference** between these techniques is that Lasso shrinks the less important feature's coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"hands-on machine learning with scikit-learn and \"这本书的作者称呼 $\\lambda$ 为 hyperparameter，他也帮我强调出来了：这个参数是learning algorithm的参数，而并不是model的参数。因此，这个参数并不受算法本身的影响，而是应该在训练之前人为设定，并且在训练中保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
